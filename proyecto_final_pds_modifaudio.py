# -*- coding: utf-8 -*-
"""Proyecto_Final_PDS_ModifAudio.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rAL-tyTOwnVvu8nIRjjKyWW4LH7-EEtC
"""

pip install mhmovie

# Template to solve the problem of editing a sequence of images to match
# the spectrogram of a song.
# This is but a very basic idea, needs plenty of improvement.

import numpy as np
import matplotlib.pyplot as plt
from scipy.io import wavfile
from IPython.display import Audio
from scipy.fft import fft, ifft, fft2, ifft2, fftshift, ifftshift
from skimage.io import imread
from cv2 import VideoWriter, VideoWriter_fourcc
from skimage.transform import resize
from skimage import util
from skimage.color import rgb2gray
from moviepy.editor import *
from statistics import mean, median
import cv2

# AUX function to change real and imaginary pairs into magnitude and phase pairs
def real_imag2magn_phas(real, imag):
    magn = np.sqrt(real*real + imag*imag)
    phas = np.arctan2(imag, real)
    return magn, phas

# AUX function to change magnitude and phase pairs into real and imaginary pairs
def magn_phas2real_imag(magnitude, phase):
    real = magnitude * np.cos(phase)
    imag = magnitude * np.sin(phase)
    return real, imag

# AUX function to show one image
def show_image(img, label):
  plt.imshow(img, cmap='gray')
  plt.title(label)
  plt.colorbar()
  plt.show()

def show_spectrum(magn, phas):
  M, N = magn.shape
  plt.figure(figsize=(11, 4))
  plt.subplot(1, 2, 1)
  plt.imshow(magn, cmap='gray', extent=(-N//2, N//2, -M//2, M//2))
  plt.colorbar()
  plt.title('Magnitude.')
  plt.subplot(1, 2, 2)
  plt.imshow(phas, cmap='gray', extent=(-N//2, N//2, -M//2, M//2))
  plt.colorbar()
  plt.title('Phase.')
  plt.show()

# Read audio file
rate, audio_data = wavfile.read("scaryMusic.wav")
audio_data=audio_data[:int(len(audio_data)/2)]
N = len(audio_data)
L = len(audio_data) / rate
print(f"Rate: {rate} Hz")
print(f"Length (n): {N}")
print(f"Length (s): {L:.2f}")

# Spit channels
channel1 = audio_data[:, 0]
channel2 = audio_data[:, 1]

plt.plot(audio_data)
plt.title('Audio sin filtrar')
print("Channel 1")
#Audio(channel1,rate=rate)
#rate es de 48000 Hz, muestras en un segundo
#print("Channel 2")
#Audio(channel2,rate=rate)

#Filtrar se침al

from scipy.signal import butter, lfilter


def butter_bandpass(lowcut, highcut, fs, order=5):
       nyq = 0.5 * fs
       low = lowcut / nyq
       high = highcut / nyq
       b, a = butter(order, [low, high], btype='band')
       return b, a


def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):
       b, a = butter_bandpass(lowcut, highcut, fs, order=order)
       y = lfilter(b, a, data)
       return y

#Filtro paso bajas
channel1F=butter_bandpass_filter(channel1,100,1500,48000)
channel2F=butter_bandpass_filter(channel2,100,1500,48000)
#Audio(channel1F,rate=rate)
audioF=np.zeros_like(audio_data)
audioF[:,0]=channel1F
audioF[:,1]=channel2F
plt.plot(audioF)
plt.title('Audio con filtro paso bajas')

#Filtro paso altas
'''
channel1F=butter_bandpass_filter(channel1,2000,7500,48000)
channel2F=butter_bandpass_filter(channel2,2000,7500,48000)
#Audio(channel1F,rate=rate)
audioF=np.zeros_like(audio_data)
audioF[:,0]=channel1F
audioF[:,1]=channel2F
plt.plot(audioF)
plt.title('Audio con filtro paso altas')'''

channel1=channel1F
channel2=channel2F

# Create a sliding window representation (split signal into N chunks of 1024 samples each, stride=100) 
#########################################################################################
#stride_size = 1024
stride_size = 512
def slice_signal(in_signal):
    return(util.view_as_windows(in_signal, window_shape=(512,), step=stride_size))
    #return(util.view_as_windows(in_signal, window_shape=(1024,), step=stride_size))

# Create a sliding window representation (split channel1 into N chunks of 1024 samples each)
intervals_chan1 = slice_signal(channel1)
intervals_chan2 = slice_signal(channel2)
print(f"Sliced channel 1: {intervals_chan1.shape}")
print(f"Sliced channel 2: {intervals_chan2.shape}")

n_intervals = intervals_chan1.shape[0]
n_intervals

# Audio spectra
X_chan1 = fft(intervals_chan1, axis=1)
magn_chan1, phas_chan1 = real_imag2magn_phas(X_chan1.real, X_chan1.imag)

X_chan2 = fft(intervals_chan2, axis=1)
magn_chan2, phas_chan2 = real_imag2magn_phas(X_chan2.real, X_chan2.imag)

max_value = max(X_chan1[1])
min_value = min(X_chan1[1])
#avg_value = mean(X_chan1[1])
print(max_value)
print(min_value)
#print(avg_value)

# Load image
img = imread('paisaje5.jpg')
img = resize(img, (512, 512))
#img = resize(img, (1024, 1024))
n_rows, n_cols, n_channs = img.shape
print(f"Number of rows: {n_rows}")
print(f"Number of columns: {n_cols}")
print(f"Number of channels: {n_channs}")
print(img.min())
print(img.max())

r_image = img.copy() # Make a copy
r_image[:,:,1] = 0
r_image[:,:,2] = 0

g_image = img.copy() # Make a copy
g_image[:,:,0] = 0
g_image[:,:,2] = 0

b_image = img.copy() # Make a copy
b_image[:,:,0] = 0
b_image[:,:,1] = 0

# Image RGB spectra
IMG = (fft2(img))
show_image(img, 'Paisaje.')
magn_IMG, phas_IMG = real_imag2magn_phas(IMG.real, IMG.imag)

for ind in range(3):
  show_spectrum(20*np.log(magn_IMG[:, :, ind]), phas_IMG[:, :, ind])

"""# Now combine them"""

# AUX function to Modulate image with sound spectrum
# Esta es la funci칩n en la que deben modelar su dise침o
def modulate(IMG, modulator):
    img_out = np.zeros_like(IMG)
    for it_chann in range(n_channs):
        img_out[:, :, it_chann] = np.multiply(modulator, IMG[:, :, it_chann])
    return(img_out)

# Genera cada frame del nuevo video
out_images = []
for ind_interval in range(n_intervals):
    IMG_tmp = np.zeros_like(IMG, dtype=complex)
    modulated = modulate(magn_IMG, magn_chan1[ind_interval, :])
    modulated = modulate(modulated, magn_chan2[ind_interval, :])
    modulated2 = modulate(phas_IMG, phas_chan1[ind_interval, :])
    modulated2 = modulate(modulated2, phas_chan2[ind_interval, :])
    # modulated=np.multiply(modulated,modulated2)
    IMG_tmp.real, IMG_tmp.imag = magn_phas2real_imag(modulated, phas_IMG)

  # Back to space domain
    #img_mod = ifft2(IMG_tmp)
    img_mod = np.real(ifft2(IMG_tmp))
    # Normalizaci칩n, puede o no ser requerida
    
    img_mod -= img_mod.min()
    if img_mod.max() != 0:
        img_mod /= img_mod.max()
    
    #img_mod = 0.6*img_mod + 0.4*img
    img_mod = np.round(255 * img_mod).astype('uint8') # Formato para guardar en disco
    #img_mod = cv2.Sobel(img_mod ,-1, dx=1, dy=0, ksize=11, scale=1, delta=0, borderType=cv2.BORDER_DEFAULT)
    out_images.append(img_mod)

# Save sequence of frames to disk
fps = (n_intervals*48000/len(audio_data))
print(fps)
out = VideoWriter('prueba Scary.mp4', VideoWriter_fourcc(*'MP4V'), 93.75, (512, 512))
for im in out_images:
    out.write(im)
out.release()

videoclip = VideoFileClip("prueba Scary.mp4")
audioclip = AudioFileClip("song.mp3")

new_audioclip = CompositeAudioClip([audioclip])
videoclip.audio = new_audioclip
videoclip.write_videofile("videoWaudioP.mp4")

background = cv2.imread('Prueba2.jpg')
background = resize(background, (512, 512))
overlay = cv2.imread('paisaje5Borrado.png')
overlay = resize(overlay, (512, 512))

rows,cols,channels = overlay.shape

combined=cv2.addWeighted(background,0.5,overlay,0.5,0)
show_image(background, 'Paisaje.')
show_image(overlay,'')
show_image(combined, 'Paisaje.')